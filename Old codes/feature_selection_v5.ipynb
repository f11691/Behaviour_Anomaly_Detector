{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65SQNsbekm3u"
   },
   "source": [
    "# Create the Fetures Group from PCAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: scapy in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/christinastodt/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scapy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyshark in /Users/christinastodt/anaconda3/envs/AnomalyTest_arm64/lib/python3.9/site-packages (0.6)\n",
      "Requirement already satisfied: lxml in /Users/christinastodt/anaconda3/envs/AnomalyTest_arm64/lib/python3.9/site-packages (from pyshark) (4.9.3)\n",
      "Requirement already satisfied: termcolor in /Users/christinastodt/anaconda3/envs/AnomalyTest_arm64/lib/python3.9/site-packages (from pyshark) (2.3.0)\n",
      "Requirement already satisfied: packaging in /Users/christinastodt/anaconda3/envs/AnomalyTest_arm64/lib/python3.9/site-packages (from pyshark) (23.2)\n",
      "Requirement already satisfied: appdirs in /Users/christinastodt/anaconda3/envs/AnomalyTest_arm64/lib/python3.9/site-packages (from pyshark) (1.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyshark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Using cached lxml-4.9.3-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Installing collected packages: lxml\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.9.3\n",
      "    Uninstalling lxml-4.9.3:\n",
      "      Successfully uninstalled lxml-4.9.3\n",
      "Successfully installed lxml-4.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml --force-reinstall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - nest_asyncio\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge/osx-arm64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyshark\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from collections import defaultdict\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "flows = defaultdict(lambda: {\"packet_count\": 0, \"total_bytes\": 0, \"start_time\": None, \"end_time\": None})\n",
    "auth_attempts = []\n",
    "\n",
    "def extract_packet_info(packet):\n",
    "    eth_src = packet.eth.src\n",
    "    eth_dst = packet.eth.dst\n",
    "    src_ip = None\n",
    "    dst_ip = None\n",
    "    src_port = None\n",
    "    dst_port = None\n",
    "    protocol = None\n",
    "    interval_between_auth_attempts = None\n",
    "    flow_identification = None\n",
    "    packet_count_in_flow = None\n",
    "    total_bytes_transmitted = None\n",
    "    traffic_direction = None\n",
    "    flow_start_timestamp = None\n",
    "    flow_end_timestamp = None\n",
    "    tcp_http_headers = None\n",
    "    http_status_codes = None\n",
    "    ssh_protocol_details = None\n",
    "    payload_size = None\n",
    "    filtered_payload_content = None\n",
    "\n",
    "    if 'IP' in packet:\n",
    "        ip = packet.ip\n",
    "        src_ip = ip.src\n",
    "        dst_ip = ip.dst\n",
    "\n",
    "        if \"TCP\" in packet or \"UDP\" in packet:\n",
    "            protocol = \"TCP\" if \"TCP\" in packet else \"UDP\"\n",
    "            layer = packet.tcp if \"TCP\" in packet else packet.udp\n",
    "            src_port = layer.srcport\n",
    "            dst_port = layer.dstport\n",
    "            flow_key = f\"{src_ip}:{src_port} -> {dst_ip}:{dst_port}\"\n",
    "            flow_identification = flow_key\n",
    "\n",
    "            # Update flow information\n",
    "            flow = flows[flow_key]\n",
    "            flow[\"packet_count\"] += 1\n",
    "            flow[\"total_bytes\"] += int(packet.length)\n",
    "            if flow[\"start_time\"] is None:\n",
    "                flow[\"start_time\"] = packet.sniff_time\n",
    "            flow[\"end_time\"] = packet.sniff_time\n",
    "\n",
    "            # Calculate values for output\n",
    "            packet_count_in_flow = flow[\"packet_count\"]\n",
    "            total_bytes_transmitted = flow[\"total_bytes\"]\n",
    "            flow_start_timestamp = flow[\"start_time\"].strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "            flow_end_timestamp = flow[\"end_time\"].strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "            traffic_direction = \"Inbound\" if src_ip == \"your_source_ip\" else \"Outbound\"\n",
    "\n",
    "            if \"HTTP\" in packet:\n",
    "                tcp_http_headers = str(packet.http)\n",
    "                http_layer = packet.http\n",
    "                request_method = getattr(http_layer, 'request_method', None)\n",
    "                request_uri = getattr(http_layer, 'request_uri', None)\n",
    "        \n",
    "                # Check for authentication attempts (example: HTTP POST to login URL)\n",
    "                if request_method == \"POST\" and request_uri and \"/login\" in request_uri:\n",
    "                    auth_attempts.append(packet.sniff_time)\n",
    "                    if len(auth_attempts) > 1:\n",
    "                        interval_between_auth_attempts = (auth_attempts[-1] - auth_attempts[-2]).total_seconds()\n",
    "\n",
    "\n",
    "        elif \"UDP\" in packet:\n",
    "            protocol = \"UDP\"\n",
    "            src_port = packet.udp.srcport\n",
    "            dst_port = packet.udp.dstport\n",
    "\n",
    "    timestamp = packet.sniff_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "\n",
    "    return {\n",
    "        \"eth_src\": eth_src,\n",
    "        \"eth_dst\": eth_dst,\n",
    "        \"src_ip\": src_ip,\n",
    "        \"dst_ip\": dst_ip,\n",
    "        \"src_port\": src_port,\n",
    "        \"dst_port\": dst_port,\n",
    "        \"protocol\": protocol,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"interval_between_auth_attempts\": interval_between_auth_attempts,\n",
    "        \"tcp_http_headers\": tcp_http_headers,\n",
    "        \"flow_identification\": flow_identification,\n",
    "        \"packet_count_in_flow\": packet_count_in_flow,\n",
    "        \"total_bytes_transmitted\": total_bytes_transmitted,\n",
    "        \"traffic_direction\": traffic_direction,\n",
    "        \"flow_start_timestamp\": flow_start_timestamp,\n",
    "        \"flow_end_timestamp\": flow_end_timestamp,\n",
    "        \"tcp_http_headers\": tcp_http_headers,\n",
    "        \"http_status_codes\": http_status_codes,\n",
    "        \"ssh_protocol_details\": ssh_protocol_details,\n",
    "        \"payload_size\": payload_size,\n",
    "        \"filtered_payload_content\": filtered_payload_content,\n",
    "    }\n",
    "\n",
    "# Function to process a list of packets\n",
    "def process_packets(packets, output_csv, first_chunk=True):\n",
    "    packet_info_list = [extract_packet_info(packet) for packet in packets]\n",
    "    df = pd.DataFrame(packet_info_list)\n",
    "    if first_chunk:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "    else:\n",
    "        df.to_csv(output_csv, index=False, mode='a', header=False)\n",
    "\n",
    "# Function to read packets from pcap in chunks and process them\n",
    "def process_pcap_in_chunks(input_pcap, output_csv, chunk_size=10000):\n",
    "    cap = pyshark.FileCapture(input_pcap)\n",
    "    packets = []\n",
    "    first_chunk = True\n",
    "    for packet in cap:\n",
    "        packets.append(packet)\n",
    "        if len(packets) == chunk_size:\n",
    "            process_packets(packets, output_csv, first_chunk)\n",
    "            packets = []\n",
    "            if first_chunk:\n",
    "                first_chunk = False\n",
    "    if packets:  # Process remaining packets if any\n",
    "        process_packets(packets, output_csv, first_chunk)\n",
    "\n",
    "# Replace 'your_input.pcap' with your pcap file path\n",
    "input_pcap = 'Tuesday-WorkingHours.pcap'\n",
    "output_csv = 'output.csv'\n",
    "process_pcap_in_chunks(input_pcap, output_csv)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN2A/LWER3VJP048v/hG0eO",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1TT0tF6I7XdTJsWx7ZJJc3ZWhklhMUbPb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AnomalyTest_arm64",
   "language": "python",
   "name": "anomalytest_arm64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
